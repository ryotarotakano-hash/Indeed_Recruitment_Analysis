# -*- coding: utf-8 -*-
import time
import random
import sys
import json
import re
import os
import urllib.parse
from DrissionPage import ChromiumPage, ChromiumOptions
from bs4 import BeautifulSoup
from groq import Groq, RateLimitError
from duckduckgo_search import DDGS

# ==========================================
# è¨­å®šã‚¨ãƒªã‚¢
# ==========================================
GROQ_API_KEY = "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"

MODEL_HEAVY = "llama-3.3-70b-versatile"
MODEL_LIGHT = "llama-3.1-8b-instant"

class TalentScopeAI:
    def __init__(self, api_key):
        if not api_key:
            print("âŒ ã‚¨ãƒ©ãƒ¼: APIã‚­ãƒ¼è¨­å®šãªã—")
            sys.exit(1)
        self.client = Groq(api_key=api_key)

    def _call_groq_safe(self, messages, model_id, response_format=None, allow_fallback=False):
        max_retries = 5
        wait_time = 20
        current_model = model_id

        for attempt in range(max_retries):
            try:
                if response_format:
                    return self.client.chat.completions.create(
                        messages=messages,
                        model=current_model,
                        temperature=0.1,
                        response_format=response_format
                    )
                else:
                    return self.client.chat.completions.create(
                        messages=messages,
                        model=current_model,
                        temperature=0.6,
                    )
            except RateLimitError:
                print(f"   â³ APIåˆ¶é™({current_model})ã€‚{wait_time}ç§’ å¾…æ©Ÿ...")
                time.sleep(wait_time)
                wait_time += 15
                if allow_fallback and current_model == MODEL_HEAVY and attempt >= 1:
                    current_model = MODEL_LIGHT
            except Exception as e:
                if "decommissioned" in str(e) or "not found" in str(e):
                    current_model = MODEL_LIGHT
                    continue
                return None
        return None

    def search_web_for_company_info(self, companies_list):
        print(f"\nğŸŒ Webæ¤œç´¢ã§æ¥­ç•Œã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å­¦ç¿’ä¸­...")
        search_results_text = ""
        with DDGS() as ddgs:
            for comp in companies_list:
                name = comp['name']
                try:
                    query = f"{name} äº‹æ¥­å†…å®¹ æ¥­ç•Œ"
                    results = list(ddgs.text(query, max_results=2))
                    info = f"â– {name}ã®æƒ…å ±:\n"
                    for r in results:
                        info += f"- {r['body']}\n"
                    search_results_text += info + "\n"
                    print(f"   ğŸ” {name}: æƒ…å ±ã‚’å–å¾—")
                    time.sleep(1)
                except Exception as e:
                    print(f"   âš ï¸ {name}ã®æ¤œç´¢å¤±æ•—: {e}")
        return search_results_text

    def generate_strict_filter(self, search_results):
        print("ğŸ§  AIãŒæ¥­ç•Œãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’ä½œæˆä¸­...")
        prompt = f"""
        Based on these search results, define the industry and negative keywords.
        Results: {search_results}
        JSON Format: {{ "target_industry": "Industry Name", "negative_keywords": "List of keywords to exclude" }}
        """
        response = self._call_groq_safe(
            messages=[{"role": "user", "content": prompt}],
            model_id=MODEL_HEAVY,
            response_format={"type": "json_object"}
        )
        try:
            return json.loads(response.choices[0].message.content)
        except:
            return {"target_industry": "General", "negative_keywords": ""}

    # ==========================================
    # â–¼ DrissionPageè¨­å®š (ã‚¹ãƒ†ãƒ«ã‚¹å¼·åŒ–ç‰ˆ)
    # ==========================================
    def _create_drission_driver(self):
        co = ChromiumOptions()
        
        # è¨˜æ†¶ï¼ˆCookieï¼‰ã‚’ä¿å­˜ã™ã‚‹ãƒ•ã‚©ãƒ«ãƒ€ã‚’è¨­å®š
        # ã“ã‚Œã«ã‚ˆã‚Šã€ä¸€åº¦çªç ´ã™ã‚Œã°æ¬¡å›ã‹ã‚‰ã€Œé¡”ãªã˜ã¿ã€ã«ãªã‚Šã¾ã™
        current_dir = os.getcwd()
        user_data_path = os.path.join(current_dir, "browser_data_stealth")
        co.set_user_data_path(user_data_path)

        # å¿…é ˆ: è‡ªå‹•åŒ–ãƒ•ãƒ©ã‚°ã‚’æ¶ˆã™
        co.set_argument('--no-sandbox')
        co.set_argument('--disable-infobars')
        co.set_argument('--lang=ja-JP')
        
        # å¿…é ˆ: æœ€æ–°ã®Chromeã¨ã—ã¦æŒ¯ã‚‹èˆã†User-Agent
        co.set_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36')

        # ãƒãƒ¼ãƒˆè¡çªå›é¿
        co.auto_port()

        try:
            page = ChromiumPage(co)
            return page
        except Exception as e:
            print(f"   âš ï¸ ãƒ–ãƒ©ã‚¦ã‚¶èµ·å‹•ã‚¨ãƒ©ãƒ¼(è‡ªå‹•ä¿®å¾©ã—ã¾ã™): {e}")
            time.sleep(3)
            return ChromiumPage(co)

    def _human_like_mouse_move(self, page):
        """ ãƒã‚¦ã‚¹ã‚’äººé–“ã®ã‚ˆã†ã«ãƒ©ãƒ³ãƒ€ãƒ ã«å‹•ã‹ã™ """
        try:
            # ç”»é¢ã®ä¸­å¤®ä»˜è¿‘ã§é©å½“ã«å‹•ã‹ã™
            width = 1920
            height = 1080
            for _ in range(3):
                x = random.randint(100, width - 100)
                y = random.randint(100, height - 100)
                # DrissionPageã«ã¯ãƒã‚¦ã‚¹ç§»å‹•ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãŒãªã„ãŸã‚
                # JSã‚’ä½¿ã£ã¦ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ç­‰ã§ä»£ç”¨ã—ã¦ã€Œæ´»å‹•ã€ã‚’ã‚¢ãƒ”ãƒ¼ãƒ«
                page.scroll.to_location(x, y)
                time.sleep(random.uniform(0.1, 0.3))
        except:
            pass

    def _solve_cloudflare(self, page):
        """ Cloudflareçªç ´ãƒ­ã‚¸ãƒƒã‚¯ï¼ˆå¼·åŒ–ç‰ˆï¼‰ """
        time.sleep(2)
        title = page.title.lower() if page.title else ""
        
        # Cloudflareã®ç”»é¢ã‹ãƒã‚§ãƒƒã‚¯
        if "verify" in title or "challenge" in title or "security" in title or page.ele("text:äººé–“ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª"):
            print("   ğŸ›¡ï¸ Cloudflareæ¤œçŸ¥ã€‚ã‚¹ãƒ†ãƒ«ã‚¹çªç ´ãƒ¢ãƒ¼ãƒ‰èµ·å‹•...")
            
            # 1. ã¾ãšäººé–“ã‚‰ã—ããƒã‚¦ã‚¹ã‚’æºã‚‰ã™ï¼ˆè¶…é‡è¦ï¼‰
            self._human_like_mouse_move(page)
            time.sleep(random.uniform(1.5, 3.0))

            # 2. iframeã®ä¸­ã®ãƒã‚§ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ã‚’æ¢ã—ã¦ã‚¯ãƒªãƒƒã‚¯
            found_checkbox = False
            for _ in range(5): # 5å›ãƒˆãƒ©ã‚¤
                try:
                    iframe = page.get_frame('@src^https://challenges.cloudflare.com')
                    if iframe:
                        # ãƒã‚§ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ã£ã½ã„è¦ç´ ã‚’å…¨æ¤œç´¢
                        # input type=checkbox ã ã‘ã§ãªãã€ã‚¯ãƒªãƒƒã‚¯å¯èƒ½ãªbodyã‚‚ç‹™ã†
                        target = iframe.ele('tag:input')
                        if not target: target = iframe.ele('@type=checkbox')
                        if not target: target = iframe.ele('tag:body') # æœ€çµ‚æ‰‹æ®µï¼šiframeå…¨ä½“ã‚’æŠ¼ã™

                        if target:
                            print("   ğŸ‘‰ è‡ªå‹•ã‚¯ãƒªãƒƒã‚¯ã‚’å®Ÿè¡Œã—ã¾ã™...")
                            target.click()
                            found_checkbox = True
                            break
                except:
                    pass
                time.sleep(1)

            # 3. çµæœå¾…ã¡ (ã‚‚ã—è‡ªå‹•ã§ãƒ€ãƒ¡ãªã‚‰ã€äººé–“ãŒæŠ¼ã™ã®ã‚’å¾…ã¤)
            # å¾…æ©Ÿæ™‚é–“ã‚’ã€3åˆ†ã€‘ã«å»¶é•·ã—ã¾ã—ãŸã€‚ã‚†ã£ãã‚Šæ‰‹å‹•ã‚¯ãƒªãƒƒã‚¯ã—ã¦ãã ã•ã„ã€‚
            print("   â³ ã€é‡è¦ã€‘èªè¨¼ç”»é¢ã§ã™ã€‚ã‚‚ã—ç”»é¢ãŒå¤‰ã‚ã‚‰ãªã„å ´åˆã€")
            print("   â³ æ‰‹å‹•ã§ã€äººé–“ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèªã€ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ãã ã•ã„ï¼(3åˆ†å¾…ã¡ã¾ã™)")
            
            start_time = time.time()
            while time.time() - start_time < 180: # 180ç§’å¾…æ©Ÿ
                title = page.title.lower() if page.title else ""
                if "verify" not in title and "challenge" not in title and "security" not in title:
                    print("   ğŸš€ çªç ´æˆåŠŸï¼ï¼ˆã¾ãŸã¯æ‰‹å‹•èªè¨¼å®Œäº†ï¼‰")
                    # æˆåŠŸã—ãŸã‚‰å°‘ã—å¾…ã£ã¦Cookieã‚’é¦´æŸ“ã¾ã›ã‚‹
                    time.sleep(3)
                    return
                time.sleep(1)
            
            print("   âš ï¸ æ™‚é–“åˆ‡ã‚Œã€‚ä»Šå›ã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")

    def extract_jobs_via_ai(self, raw_html, company, location, filter_data):
        print(f"   ğŸ¤– Groqè§£æä¸­... ({company})")
        soup = BeautifulSoup(raw_html, "html.parser")

        extracted_jobs_text = ""
        
        job_titles = soup.find_all("h2", class_=lambda x: x and "jobTitle" in x)
        if not job_titles:
            job_links = soup.find_all("a", href=True)
            candidates = [a for a in job_links if "jk=" in a['href'] or "/rc/clk" in a['href']]
        else:
            candidates = [h2.find("a") for h2 in job_titles if h2.find("a")]

        for i, a_tag in enumerate(candidates):
            if not a_tag: continue
            
            href = a_tag.get('href', '')
            jk_id = ""
            if "jk=" in href:
                match = re.search(r'jk=([a-zA-Z0-9]+)', href)
                if match: jk_id = match.group(1)
            
            stable_url = f"https://jp.indeed.com/viewjob?jk={jk_id}" if jk_id else "URL_NOT_FOUND"
            title = a_tag.get_text(strip=True)
            
            card = a_tag.find_parent("div", class_=lambda x: x and "card" in x.lower()) 
            if not card: card = a_tag.find_parent("td")
            if not card: card = a_tag.find_parent("div")

            card_text = card.get_text(separator=" | ", strip=True) if card else ""
            
            extracted_jobs_text += f"""
            [JOB_BLOCK_{i+1}]
            Title: {title}
            URL: {stable_url}
            RawContent: {card_text}
            --------------------------------
            """

        if not extracted_jobs_text:
            print("   âš ï¸ æ§‹é€ åŒ–æŠ½å‡ºå¤±æ•—ã€‚å…¨æ–‡ãƒ¢ãƒ¼ãƒ‰ã¸ã€‚")
            for tag in soup(["script", "style", "svg", "path", "footer", "nav", "noscript", "header"]):
                tag.decompose()
            extracted_jobs_text = soup.get_text(separator=" ", strip=True)[:18000]

        location_instruction = ""
        if location:
            location_instruction = f"4. LOCATION: Prioritize jobs in '{location}', but if the job is clearly for {company}, include it."

        prompt = f"""
        Extract job postings for "{company}" from the text.
        FILTERING RULES:
        1. Industry: {filter_data['target_industry']}
        2. Exclude keywords: {filter_data['negative_keywords']}
        3. Exclude Hotel/Clinic staff unless target is one.
        {location_instruction}
        
        Return JSON list: 
        [
          {{
            "title": "Job Title",
            "url": "URL found in block", 
            "salary": "Salary text",
            "location": "Location",
            "remote": "Remote Info",
            "details": "Summary"
          }}
        ]
        If no relevant jobs found, return [].
        
        TEXT BLOCKS:
        {extracted_jobs_text[:25000]}
        """

        response = self._call_groq_safe(
            messages=[{"role": "user", "content": prompt}],
            model_id=MODEL_LIGHT, 
            response_format={"type": "json_object"}
        )

        if not response: return []
        try:
            data = json.loads(response.choices[0].message.content)
            if isinstance(data, dict):
                for key in data:
                    if isinstance(data[key], list): return data[key]
                return [] 
            return data
        except:
            return []

    def _extract_prefecture(self, address):
        if not address: return None
        match = re.search(r'(.+?[éƒ½é“åºœçœŒ])', address)
        if match: return match.group(1)
        return None

    def run_single_search(self, company_info, filter_data):
        raw_name = company_info['name']
        original_loc = company_info['loc']

        clean_name = raw_name
        search_loc = original_loc
        if not search_loc and (" " in raw_name or "ã€€" in raw_name):
            parts = re.split(r'[ ã€€]', raw_name)
            if any(x in parts[1] for x in ["éƒ½", "é“", "åºœ", "çœŒ", "å¸‚", "åŒº"]):
                print(f"   ğŸ’¡ ä½æ‰€è‡ªå‹•æ¤œå‡º: {parts[1]}")
                clean_name = parts[0]
                search_loc = parts[1]

        strategies = []
        strategies.append({"q": clean_name, "l": search_loc, "desc": "æŒ‡å®šã‚¨ãƒªã‚¢"})
        if search_loc:
            pref = self._extract_prefecture(search_loc)
            if pref and pref != search_loc:
                strategies.append({"q": clean_name, "l": pref, "desc": "éƒ½é“åºœçœŒ"})
        strategies.append({"q": clean_name, "l": None, "desc": "å…¨å›½"})

        MAX_RETRIES = 2 

        for strategy in strategies:
            q_val = strategy["q"]
            l_val = strategy["l"]
            desc = strategy["desc"]

            for attempt in range(MAX_RETRIES):
                page = None
                try:
                    retry_label = f" ({desc} - è©¦è¡Œ{attempt+1})"
                    print(f"ğŸ” '{q_val}' ã‚’æ¤œç´¢ä¸­... ã‚¨ãƒªã‚¢: {l_val if l_val else 'å…¨å›½'}{retry_label}")
                    
                    page = self._create_drission_driver()
                    
                    # æˆ¦ç•¥: ã¾ãšãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ã«è¡Œã£ã¦ã€äººé–“ã‚¢ãƒ”ãƒ¼ãƒ«ã‚’ã™ã‚‹
                    if attempt == 0:
                        page.get("https://jp.indeed.com/")
                        self._solve_cloudflare(page)
                    
                    # æœ¬ç•ªæ¤œç´¢
                    base_url = f"https://jp.indeed.com/jobs?q={urllib.parse.quote(q_val)}"
                    if l_val:
                        base_url += f"&l={urllib.parse.quote(l_val)}"
                    
                    page.get(base_url)
                    
                    # å†åº¦ãƒã‚§ãƒƒã‚¯
                    self._solve_cloudflare(page)
                    
                    # èª­ã¿è¾¼ã¿å¾…æ©Ÿ
                    time.sleep(2)
                    page.scroll.to_bottom()
                    time.sleep(1)
                    
                    html_content = page.html
                    jobs_data = self.extract_jobs_via_ai(html_content, q_val, l_val, filter_data)
                    
                    page.quit() # çµ‚ã‚ã£ãŸã‚‰é–‰ã˜ã‚‹
                    
                    if jobs_data is not None and len(jobs_data) > 0:
                        print(f"   âœ… ãƒ’ãƒƒãƒˆã—ã¾ã—ãŸï¼ ({len(jobs_data)}ä»¶)")
                        
                        formatted_jobs = ""
                        count = 0
                        for job in jobs_data[:10]:
                            if isinstance(job, dict):
                                t = job.get('title', 'ä¸æ˜')
                                u = job.get('url', '#')
                                sal = job.get('salary', 'ãªã—')
                                l = job.get('location', 'ãªã—')
                                r = job.get('remote', 'ä¸æ˜')
                                d = job.get('details', '')
                                formatted_jobs += f"JOB_START\nTitle:{t}\nURL:{u}\nSalary:{sal}\nLoc:{l}\nRem:{r}\nDet:{d}\nJOB_END\n"
                                count += 1
                        
                        return {"count": len(jobs_data), "jobs": formatted_jobs, "raw_data": jobs_data}
                    
                    else:
                        print(f"   âš ï¸ æ±‚äººãªã— (æ¬¡ã®æˆ¦ç•¥ã¸)")
                        continue 

                except Exception as e:
                    print(f"   âš ï¸ ã‚¨ãƒ©ãƒ¼: {e}")
                    time.sleep(3)
                    continue 

        return None

    def analyze_with_groq(self, company_data_list, companies_info, filter_data):
        print("\nğŸ§  Groqã§æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆä¸­...")
        input_data_str = ""
        for comp in companies_info:
            name = comp['name'] 
            loc_req = comp['loc']
            data = company_data_list.get(name)
            
            input_data_str += f"\n### å¯¾è±¡ä¼æ¥­: {name} (å¸Œæœ›ã‚¨ãƒªã‚¢: {loc_req if loc_req else 'æŒ‡å®šãªã—/è‡ªå‹•æ¤œå‡º'})\n"
            if data and data['count'] > 0:
                input_data_str += f"æ¤œå‡ºæ•°: {data['count']}\n{data['jobs']}\n"
            else:
                input_data_str += f"ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: è©²å½“æ±‚äººãªã—\n"
            input_data_str += "-"*10

        prompt = f"""
        ã‚ãªãŸã¯æ¡ç”¨ã‚¢ãƒŠãƒªã‚¹ãƒˆã§ã™ã€‚ä»¥ä¸‹ã®ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
        ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæ¥­ç•Œ: {filter_data['target_industry']}
        
        ã€é‡è¦æŒ‡ç¤º: å‡ºåŠ›å½¢å¼ã€‘
        1. ãƒ—ãƒ¬ãƒ¼ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ä½¿ç”¨ (Markdownè¨˜å·ãªã—)ã€‚
        2. ä»¥ä¸‹ã®ãƒ†ãƒ³ãƒ—ãƒ¬ã«å®Œå…¨ã«æº–æ‹ ã™ã‚‹ã“ã¨ã€‚
        3. ã€Œ8. æ±‚äººç¥¨ãƒªãƒ³ã‚¯ã€ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æœ€å¾Œã«è¿½åŠ ã—ã€ãã“ã«URLã‚’ã¾ã¨ã‚ã‚‹ã“ã¨ã€‚
        
        ã€å‡ºåŠ›ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã€‘
        
        ã€ä¼æ¥­åã€‘ (ã‚¨ãƒªã‚¢: [ã‚¨ãƒªã‚¢])
        
        1. çµ¦æ–™
           [è·ç¨®å] : [é‡‘é¡]
           [è·ç¨®å] : [é‡‘é¡]
        
        2. ç¦åˆ©åšç”Ÿ
           [å†…å®¹]
        
        3. è¨´æ±‚ãƒã‚¤ãƒ³ãƒˆ
           [å†…å®¹]
        
        4. å‹¤å‹™åœ°
           [å†…å®¹]
        
        5. å‹¤å‹™æ™‚é–“
           [å†…å®¹]
        
        6. ãƒªãƒ¢ãƒ¼ãƒˆãƒ¯ãƒ¼ã‚¯
           [å†…å®¹]
        
        7. æ¥­å‹™å†…å®¹ãƒ»æ¡ˆä»¶
           [å†…å®¹]
        
        8. æ±‚äººç¥¨ãƒªãƒ³ã‚¯
           [è·ç¨®å] : [URL]
           [è·ç¨®å] : [URL]
        
        --------------------------------------------------
        
        ã€ãƒ‡ãƒ¼ã‚¿ã€‘
        {input_data_str}
        """
        response = self._call_groq_safe(
            messages=[{"role": "user", "content": prompt}],
            model_id=MODEL_HEAVY,
            allow_fallback=True 
        )
        text = response.choices[0].message.content if response else "âŒ ç”Ÿæˆå¤±æ•—"
        
        clean_text = text.replace("**", "").replace("##", "â– ").replace("###", "â– ").replace("* ", "ãƒ»")
        clean_text = re.sub(r'^\s*-\s', 'ãƒ»', clean_text, flags=re.MULTILINE)
        
        return clean_text

def main():
    print("=========================================")
    print("   TalentScope AI - Final Stealth Ver    ")
    print("=========================================")
    
    input_str = input("ä¼æ¥­ãƒªã‚¹ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ (ä¾‹: æ ªå¼ä¼šç¤¾ã‚¨ãƒ¬ãƒ•ã‚¡ãƒ³ãƒˆã‚¹ãƒˆãƒ¼ãƒ³@æ±äº¬éƒ½æ¸‹è°·åŒº)\n> ")
    input_str = input_str.replace("ï¼Œ", ",").replace("ã€€", " ").strip()
    
    raw_list = [x.strip() for x in input_str.split(",") if x.strip()]
    companies_info = []
    
    for item in raw_list:
        if "@" in item:
            parts = item.split("@")
            companies_info.append({"name": parts[0].strip(), "loc": parts[1].strip()})
        else:
            companies_info.append({"name": item, "loc": None})
            
    if not companies_info: return

    analyzer = TalentScopeAI(api_key=GROQ_API_KEY)
    
    web_info = analyzer.search_web_for_company_info(companies_info)
    filter_data = analyzer.generate_strict_filter(web_info)
    
    results = {}
    for comp in companies_info:
        if len(results) > 0:
            print("   â˜• æ¬¡ã®æ¤œç´¢ã¸...")
            time.sleep(2)
        
        data = analyzer.run_single_search(comp, filter_data)
        results[comp['name']] = data

    if results:
        report = analyzer.analyze_with_groq(results, companies_info, filter_data)
        print("\n" + "="*50)
        print("          åˆ†æãƒ¬ãƒãƒ¼ãƒˆçµæœ")
        print("="*50 + "\n")
        print(report)

if __name__ == "__main__":
    main()
