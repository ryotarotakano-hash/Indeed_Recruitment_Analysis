import time
import random
import sys
import json
import re
import urllib.parse
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
from groq import Groq, RateLimitError
from duckduckgo_search import DDGS

# ==========================================
# è¨­å®šã‚¨ãƒªã‚¢
# ==========================================
GROQ_API_KEY = "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"

MODEL_HEAVY = "llama-3.3-70b-versatile"
MODEL_LIGHT = "llama-3.1-8b-instant"

class TalentScopeAI:
    def __init__(self, api_key):
        if not api_key:
            print("âŒ ã‚¨ãƒ©ãƒ¼: APIã‚­ãƒ¼è¨­å®šãªã—")
            sys.exit(1)
        self.client = Groq(api_key=api_key)

    def _call_groq_safe(self, messages, model_id, response_format=None, allow_fallback=False):
        max_retries = 5
        wait_time = 20
        current_model = model_id

        for attempt in range(max_retries):
            try:
                if response_format:
                    return self.client.chat.completions.create(
                        messages=messages,
                        model=current_model,
                        temperature=0.1,
                        response_format=response_format
                    )
                else:
                    return self.client.chat.completions.create(
                        messages=messages,
                        model=current_model,
                        temperature=0.6,
                    )
            except RateLimitError:
                print(f"   â³ APIåˆ¶é™({current_model})ã€‚{wait_time}ç§’ å¾…æ©Ÿ...")
                time.sleep(wait_time)
                wait_time += 15
                if allow_fallback and current_model == MODEL_HEAVY and attempt >= 1:
                    current_model = MODEL_LIGHT
            except Exception as e:
                if "decommissioned" in str(e) or "not found" in str(e):
                    current_model = MODEL_LIGHT
                    continue
                return None
        return None

    def search_web_for_company_info(self, companies_list):
        print(f"\nğŸŒ Webæ¤œç´¢ã§æ¥­ç•Œã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å­¦ç¿’ä¸­...")
        search_results_text = ""
        with DDGS() as ddgs:
            for comp in companies_list:
                name = comp['name']
                try:
                    query = f"{name} äº‹æ¥­å†…å®¹ æ¥­ç•Œ"
                    results = list(ddgs.text(query, max_results=2))
                    info = f"â– {name}ã®æƒ…å ±:\n"
                    for r in results:
                        info += f"- {r['body']}\n"
                    search_results_text += info + "\n"
                    print(f"   ğŸ” {name}: æƒ…å ±ã‚’å–å¾—")
                    time.sleep(1)
                except Exception as e:
                    print(f"   âš ï¸ {name}ã®æ¤œç´¢å¤±æ•—: {e}")
        return search_results_text

    def generate_strict_filter(self, search_results):
        print("ğŸ§  AIãŒæ¥­ç•Œãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’ä½œæˆä¸­...")
        prompt = f"""
        Based on these search results, define the industry and negative keywords.
        Results: {search_results}
        JSON Format: {{ "target_industry": "Industry Name", "negative_keywords": "List of keywords to exclude" }}
        """
        response = self._call_groq_safe(
            messages=[{"role": "user", "content": prompt}],
            model_id=MODEL_HEAVY,
            response_format={"type": "json_object"}
        )
        try:
            return json.loads(response.choices[0].message.content)
        except:
            return {"target_industry": "General", "negative_keywords": ""}

    def _create_fresh_driver(self):
        options = Options()
        
        # ==========================================
        # â–¼ ã‚¯ãƒ©ã‚¦ãƒ‰ã‚µãƒ¼ãƒãƒ¼ç”¨è¨­å®šï¼ˆãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ãƒ¢ãƒ¼ãƒ‰ï¼‰
        # ==========================================
        options.add_argument("--headless")  # ç”»é¢ã‚’è¡¨ç¤ºã—ãªã„
        options.add_argument("--no-sandbox") # ã‚µãƒ³ãƒ‰ãƒœãƒƒã‚¯ã‚¹è§£é™¤
        options.add_argument("--disable-dev-shm-usage") # ãƒ¡ãƒ¢ãƒªå…±æœ‰ç„¡åŠ¹åŒ–
        options.add_argument("--disable-gpu") # GPUç„¡åŠ¹åŒ–
        # ==========================================

        options.add_argument("--start-maximized")
        options.add_argument("--disable-blink-features=AutomationControlled")
        options.add_experimental_option("excludeSwitches", ["enable-automation"])
        options.add_experimental_option('useAutomationExtension', False)
        # User-Agentã‚’å½è£…ã—ã¦ãƒ–ãƒ­ãƒƒã‚¯ã‚’å›é¿ã—ã‚„ã™ãã™ã‚‹
        options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36")
        options.add_argument("--log-level=3")
        
        service = Service(ChromeDriverManager().install())
        return webdriver.Chrome(service=service, options=options)

    def _scroll_page(self, driver):
        try:
            for _ in range(4):
                driver.find_element(By.TAG_NAME, 'body').send_keys(Keys.PAGE_DOWN)
                time.sleep(random.uniform(0.5, 1.2))
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(1)
        except: pass

    def extract_jobs_via_ai(self, raw_html, company, location, filter_data):
        print(f"   ğŸ¤– Groqè§£æä¸­... ({company})")
        soup = BeautifulSoup(raw_html, "html.parser")

        # æ±‚äººã‚«ãƒ¼ãƒ‰ç‰¹å®š & æ§‹é€ åŒ–
        extracted_jobs_text = ""
        
        job_titles = soup.find_all("h2", class_=lambda x: x and "jobTitle" in x)
        if not job_titles:
            job_links = soup.find_all("a", href=True)
            candidates = [a for a in job_links if "jk=" in a['href'] or "/rc/clk" in a['href']]
        else:
            candidates = [h2.find("a") for h2 in job_titles if h2.find("a")]

        for i, a_tag in enumerate(candidates):
            if not a_tag: continue
            
            href = a_tag.get('href', '')
            jk_id = ""
            if "jk=" in href:
                match = re.search(r'jk=([a-zA-Z0-9]+)', href)
                if match: jk_id = match.group(1)
            
            stable_url = f"https://jp.indeed.com/viewjob?jk={jk_id}" if jk_id else "URL_NOT_FOUND"
            title = a_tag.get_text(strip=True)
            
            card = a_tag.find_parent("div", class_=lambda x: x and "card" in x.lower()) 
            if not card: card = a_tag.find_parent("td")
            if not card: card = a_tag.find_parent("div")

            card_text = card.get_text(separator=" | ", strip=True) if card else ""
            
            extracted_jobs_text += f"""
            [JOB_BLOCK_{i+1}]
            Title: {title}
            URL: {stable_url}
            RawContent: {card_text}
            --------------------------------
            """

        if not extracted_jobs_text:
            print("   âš ï¸ æ§‹é€ åŒ–æŠ½å‡ºå¤±æ•—ã€‚å…¨æ–‡ãƒ¢ãƒ¼ãƒ‰ã«åˆ‡ã‚Šæ›¿ãˆã¾ã™ã€‚")
            for tag in soup(["script", "style", "svg", "path", "footer", "nav", "noscript", "header"]):
                tag.decompose()
            extracted_jobs_text = soup.get_text(separator=" ", strip=True)[:18000]

        location_instruction = ""
        if location:
            location_instruction = f"4. LOCATION: Prioritize jobs in '{location}', but if the job is clearly for {company}, include it."

        prompt = f"""
        Extract job postings for "{company}" from the text.
        FILTERING RULES:
        1. Industry: {filter_data['target_industry']}
        2. Exclude keywords: {filter_data['negative_keywords']}
        3. Exclude Hotel/Clinic staff unless target is one.
        {location_instruction}
        
        Return JSON list: 
        [
          {{
            "title": "Job Title",
            "url": "URL found in block", 
            "salary": "Salary text",
            "location": "Location",
            "remote": "Remote Info",
            "details": "Summary"
          }}
        ]
        If no relevant jobs found, return [].
        
        TEXT BLOCKS:
        {extracted_jobs_text[:25000]}
        """

        response = self._call_groq_safe(
            messages=[{"role": "user", "content": prompt}],
            model_id=MODEL_LIGHT, 
            response_format={"type": "json_object"}
        )

        if not response: return []
        try:
            data = json.loads(response.choices[0].message.content)
            if isinstance(data, dict):
                for key in data:
                    if isinstance(data[key], list): return data[key]
                return [] 
            return data
        except:
            return []

    def _extract_prefecture(self, address):
        if not address: return None
        match = re.search(r'(.+?[éƒ½é“åºœçœŒ])', address)
        if match: return match.group(1)
        return None

    def run_single_search(self, company_info, filter_data):
        raw_name = company_info['name']
        original_loc = company_info['loc']

        clean_name = raw_name
        search_loc = original_loc
        if not search_loc and (" " in raw_name or "ã€€" in raw_name):
            parts = re.split(r'[ ã€€]', raw_name)
            if any(x in parts[1] for x in ["éƒ½", "é“", "åºœ", "çœŒ", "å¸‚", "åŒº"]):
                print(f"   ğŸ’¡ ä½æ‰€è‡ªå‹•æ¤œå‡º: {parts[1]}")
                clean_name = parts[0]
                search_loc = parts[1]

        strategies = []
        strategies.append({"q": clean_name, "l": search_loc, "desc": "æŒ‡å®šã‚¨ãƒªã‚¢"})
        if search_loc:
            pref = self._extract_prefecture(search_loc)
            if pref and pref != search_loc:
                strategies.append({"q": clean_name, "l": pref, "desc": "éƒ½é“åºœçœŒ"})
        strategies.append({"q": clean_name, "l": None, "desc": "å…¨å›½"})

        MAX_RETRIES = 2 

        for strategy in strategies:
            q_val = strategy["q"]
            l_val = strategy["l"]
            desc = strategy["desc"]

            for attempt in range(MAX_RETRIES):
                driver = None
                try:
                    retry_label = f" ({desc} - è©¦è¡Œ{attempt+1})"
                    print(f"ğŸ” '{q_val}' ã‚’æ¤œç´¢ä¸­... ã‚¨ãƒªã‚¢: {l_val if l_val else 'å…¨å›½'}{retry_label}")
                    
                    driver = self._create_fresh_driver()
                    base_url = f"https://jp.indeed.com/jobs?q={urllib.parse.quote(q_val)}"
                    if l_val:
                        base_url += f"&l={urllib.parse.quote(l_val)}"
                    
                    driver.get(base_url)
                    
                    page_src = driver.page_source.lower()
                    if "verify you are human" in page_src or "challenge" in driver.title.lower() or "security check" in page_src:
                        print("   âš ï¸ ãƒ–ãƒ­ãƒƒã‚¯æ¤œçŸ¥ã€‚å†èµ·å‹•ã—ã¾ã™...")
                        driver.quit()
                        time.sleep(10)
                        continue 
                    
                    time.sleep(3)
                    self._scroll_page(driver)
                    
                    jobs_data = self.extract_jobs_via_ai(driver.page_source, q_val, l_val, filter_data)
                    
                    if jobs_data is not None and len(jobs_data) > 0:
                        print(f"   âœ… ãƒ’ãƒƒãƒˆã—ã¾ã—ãŸï¼ ({len(jobs_data)}ä»¶)")
                        driver.quit()
                        
                        formatted_jobs = ""
                        count = 0
                        for job in jobs_data[:10]: # å¤šã‚ã«å–å¾—
                            if isinstance(job, dict):
                                t = job.get('title', 'ä¸æ˜')
                                u = job.get('url', '#')
                                sal = job.get('salary', 'ãªã—')
                                l = job.get('location', 'ãªã—')
                                r = job.get('remote', 'ä¸æ˜')
                                d = job.get('details', '')
                                formatted_jobs += f"JOB_START\nTitle:{t}\nURL:{u}\nSalary:{sal}\nLoc:{l}\nRem:{r}\nDet:{d}\nJOB_END\n"
                                count += 1
                        
                        # raw_dataã‚’è¿”ã™ã®ã§ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã®è¡¨ã«å¯¾å¿œ
                        return {"count": len(jobs_data), "jobs": formatted_jobs, "raw_data": jobs_data}
                    
                    else:
                        print(f"   âš ï¸ æ±‚äººãªã— (æ¬¡ã®æˆ¦ç•¥ã¸)")
                        driver.quit()
                        break 

                except Exception as e:
                    print(f"   âš ï¸ ã‚¨ãƒ©ãƒ¼: {e}")
                    if driver: driver.quit()
                    time.sleep(5)
                    continue 

        return None

    def analyze_with_groq(self, company_data_list, companies_info, filter_data):
        print("\nğŸ§  Groqã§æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆä¸­ (Template v33)...")
        input_data_str = ""
        for comp in companies_info:
            name = comp['name'] 
            loc_req = comp['loc']
            data = company_data_list.get(name)
            
            input_data_str += f"\n### å¯¾è±¡ä¼æ¥­: {name} (å¸Œæœ›ã‚¨ãƒªã‚¢: {loc_req if loc_req else 'æŒ‡å®šãªã—/è‡ªå‹•æ¤œå‡º'})\n"
            if data and data['count'] > 0:
                input_data_str += f"æ¤œå‡ºæ•°: {data['count']}\n{data['jobs']}\n"
            else:
                input_data_str += f"ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: è©²å½“æ±‚äººãªã—\n"
            input_data_str += "-"*10

        prompt = f"""
        ã‚ãªãŸã¯æ¡ç”¨ã‚¢ãƒŠãƒªã‚¹ãƒˆã§ã™ã€‚ä»¥ä¸‹ã®ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
        ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæ¥­ç•Œ: {filter_data['target_industry']}
        
        ã€é‡è¦æŒ‡ç¤º: å‡ºåŠ›å½¢å¼ã€‘
        1. ãƒ—ãƒ¬ãƒ¼ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ä½¿ç”¨ (Markdownè¨˜å·ãªã—)ã€‚
        2. ä»¥ä¸‹ã®ãƒ†ãƒ³ãƒ—ãƒ¬ã«å®Œå…¨ã«æº–æ‹ ã™ã‚‹ã“ã¨ã€‚
        3. ã€Œ8. æ±‚äººç¥¨ãƒªãƒ³ã‚¯ã€ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æœ€å¾Œã«è¿½åŠ ã—ã€ãã“ã«URLã‚’ã¾ã¨ã‚ã‚‹ã“ã¨ã€‚
        
        ã€å‡ºåŠ›ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã€‘
        
        ã€ä¼æ¥­åã€‘ (ã‚¨ãƒªã‚¢: [ã‚¨ãƒªã‚¢])
        
        1. çµ¦æ–™
           [è·ç¨®å] : [é‡‘é¡]
           [è·ç¨®å] : [é‡‘é¡]
        
        2. ç¦åˆ©åšç”Ÿ
           [å†…å®¹]
        
        3. è¨´æ±‚ãƒã‚¤ãƒ³ãƒˆ
           [å†…å®¹]
        
        4. å‹¤å‹™åœ°
           [å†…å®¹]
        
        5. å‹¤å‹™æ™‚é–“
           [å†…å®¹]
        
        6. ãƒªãƒ¢ãƒ¼ãƒˆãƒ¯ãƒ¼ã‚¯
           [å†…å®¹]
        
        7. æ¥­å‹™å†…å®¹ãƒ»æ¡ˆä»¶
           [å†…å®¹]
        
        8. æ±‚äººç¥¨ãƒªãƒ³ã‚¯
           [è·ç¨®å] : [URL]
           [è·ç¨®å] : [URL]
        
        --------------------------------------------------
        
        ã€ãƒ‡ãƒ¼ã‚¿ã€‘
        {input_data_str}
        """
        response = self._call_groq_safe(
            messages=[{"role": "user", "content": prompt}],
            model_id=MODEL_HEAVY,
            allow_fallback=True 
        )
        text = response.choices[0].message.content if response else "âŒ ç”Ÿæˆå¤±æ•—"
        
        # ä»•ä¸Šã’ã®æƒé™¤
        clean_text = text.replace("**", "").replace("##", "â– ").replace("###", "â– ").replace("* ", "ãƒ»")
        clean_text = re.sub(r'^\s*-\s', 'ãƒ»', clean_text, flags=re.MULTILINE)
        
        return clean_text

def main():
    print("=========================================")
    print("   TalentScope AI - v33 (Server Mode)")
    print("=========================================")
    
    input_str = input("ä¼æ¥­ãƒªã‚¹ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ (ä¾‹: æ ªå¼ä¼šç¤¾ã‚¨ãƒ¬ãƒ•ã‚¡ãƒ³ãƒˆã‚¹ãƒˆãƒ¼ãƒ³@æ±äº¬éƒ½æ¸‹è°·åŒº)\n> ")
    input_str = input_str.replace("ï¼Œ", ",").replace("ã€€", " ").strip()
    
    raw_list = [x.strip() for x in input_str.split(",") if x.strip()]
    companies_info = []
    
    for item in raw_list:
        if "@" in item:
            parts = item.split("@")
            companies_info.append({"name": parts[0].strip(), "loc": parts[1].strip()})
        else:
            companies_info.append({"name": item, "loc": None})
            
    if not companies_info: return

    analyzer = TalentScopeAI(api_key=GROQ_API_KEY)
    
    web_info = analyzer.search_web_for_company_info(companies_info)
    filter_data = analyzer.generate_strict_filter(web_info)
    
    results = {}
    for comp in companies_info:
        if len(results) > 0:
            print("   â˜• æ¬¡ã®æ¤œç´¢ã¸...")
            time.sleep(2)
        
        data = analyzer.run_single_search(comp, filter_data)
        results[comp['name']] = data

    if results:
        report = analyzer.analyze_with_groq(results, companies_info, filter_data)
        print("\n" + "="*50)
        print("          åˆ†æãƒ¬ãƒãƒ¼ãƒˆçµæœ")
        print("="*50 + "\n")
        print(report)

if __name__ == "__main__":
    main()
